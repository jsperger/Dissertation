---
output: pdf_document
---

Maximizing the expected number of successes falls under the general umbrella of maximizing the cumulative expected reward $\R_n = \E[\sum_{t = 1}^n Y_t]$. This objective is commonly restated as minimizing the cumulative regret, where regret is defined as the difference in expectation between the action we chose and the action chosen by an oracle with knowledge of the best treatment in expectation. The expected number of successes and expected regret both have Bayesian and Frequentist versions which differ in terms of whether the regret is defined with respect to fixed parameter(s) (Frequentist) or a distribution over the parameter space (Bayesian). Notice that maximizing the cumulative expected rewards only considers the outcomes of those within the trial. There are two commonly used contrasting objectives that only consider the outcomes after the trial is complete: 1) the expected reward for the first out-of-sample patient, known as the "simple" regret, and 2) the posterior probability assigned to the hypothesis that an arm is optimal for the true optimal arm. A less common, but no less important, objective is maximizing the power of a statistical test (the specifics of the test will depend on the problem at hand).  

For all objectives, there is work on instance-dependent (or problem-dependent) bounds that characterize optimal performance on a specific bandit instance and instance-independent bounds that provide more general performance guarantees that don't rely on parametric assumptions. While instance-independent bounds don't rely on parametric assumptions, they are still a family of results and the bounds for the same algorithm and general problem may vary depending on what assumptions are made such as boundedness or the tail probabilities (e.g. sub-Gaussian, sub-exponential). We can additionally distinguish between asymptotic and finite-sample results. 

Regret may be analyzed asymptotically as $n$ goes to infinity or for a finite time horizon. Asymptotically we wish to find a policy $\pi$ that for all bandits $\nu$ satisfies $\lim_{n \to \infty} \frac{R_n(\pi, \nu)}{f(n)} = 0$ where $f(n)$ is as small as possible. In the finite-horizon case we will instead seek guarantees that with high probability the regret will not grow faster than a certain rate with the sample size. Asymptotic results do not provide any guarantees about the finite-time performance of an algorithm; you could take an algorithm and add a fixed period where we throw out the data without looking at it without changing the asymptotic performance. Still, favorable asymptotic results are encouraging that the algorithm may have good finite-time performance. 

We will now formally define the regret of a policy or bandit algorithm. Let $\nu = \{P_a: a \in \mathcal{A}\}$ be a collection of distributions (a stochastic bandit), and let $\mu_a(\nu)$ denote the expected reward of playing arm $a$, $\mu_a(\nu) = \int_{-\infty}^{\infty} x \, \mathrm{d}P_a(x)$. Define $\mu^*(\nu) = \max_{a \in \mathcal{A}} \mu_a(\nu)$ as the mean reward of the optimal arm.  Then the regret of policy $\pi$ on bandit instance $\nu$ is given by 
$$\Reg_n(\nu, \pi) = n\mu^*(\nu) - \E_{\pi} \left[\sum_{t = 1}^n Y_t \right]$$ 

This can be rewritten in terms of the gaps between the mean of each arm and the optimal arm and the expected number of times that the arm will be assigned provided there is a countable number of arms and a finite time horizon. Define the suboptimality gap for arm $a$ as $\Delta_a(\nu) = \mu^*(\nu) - \mu_a(\nu)$, and let the sample size for arm $a$ after $n$ patients have entered the study be denoted by $N_a(n) = \sum_{t = 1}^n I(A_t = a)$. Then we can rewrite the regret

$$\Reg_n = \sum_{a \in \mathcal{A}}\Delta_a \E_{\pi}[N_a(n)]$$

---
output: pdf_document
---


## The Relationship Between Power and Regret

One might hope that by increasing the expected number of successes within the trial we are also increasing the probability that we can identify the best treatment after the trial. Sadly this is not the case. Adaptive exploration methods that try to maximize the expected number of successes in a trial do so at the expense of power in the two-arm case \citep{bubeck2009pure, wathen2017simulation}. The power of a statistical test is the probability of rejecting the null hypothesis using that test for a fixed alternative parameter value. Power is contingent on the statistical test in question. Consider an experiment that can be represented using the $K$-armed MAB problem and the two null hypotheses we might wish to test after running an experiment, hypothesis A $H_0: m_1 = \ldots = \mu_K$ and hypothesis B $H_0: m_1 > m_2$. A design that assigned all patients equally between Treatments 1 and 2 would increase the power to test Hypothesis B relative to a design that split patients equally between Treatments 1, ..., K while decreasing the power to test Hypothesis A. For realistic experiments, designs that maximize the power to test all potential hypotheses related to the study outcomes are not possible. The impact on power is more complex when there are multiple treatments because there are multiple potential hypotheses of interest including pair-wise comparisons and a global hypothesis that all the arms' means are equal. The power to compare the best and second-best treatment may increase because the sample size for these arms could be higher than under equal allocation, but the power for comparisons between arms that are sub-optimal will likely be worse than under equal allocation. The conflict in what's being optimized remains, and methods that explicitly try to maximize power when there are multiple treatment arms will outperform TS at the expense of the expected number of successes in the trial.

To help understand the intuition for why methods that optimize the expected number of successes are suboptimal, consider Neyman's classic design for allocating samples when the population can be divided into mutually exclusive strata and variance within each strata is known; in the case of a clinical trial without covariates the treatment arms form the strata. Initially developed in the context of survey sampling, for clinical trials, Neyman allocation \cite{neyman1934aspects} assigns patients to treatments proportional to the treatment's variance over the sum of all treatments' variances. It can be proven that this allocation is optimal for minimizing the variance of the estimator for the difference in treatment effect provided the resulting allocation is an integer solution (rounding to reach an integer solution may not be optimal). For a $k$-arm trial where the responses follow Gaussian distributions with mean $\mu_k$ and standard deviation $\sigma_k$ and where a Wald test will be used to test for a difference in treatment effect, the optimal allocation ratio $\rho$ of patients assigned to treatment $i$ is 

$$\rho^*_{\mathrm{Neyman}}(a_i) = \frac{\sigma_i}{\sum_{k = 1}^{K} \sigma_k}$$
---
output: pdf_document
---

# Terminology

Before diving in, a brief note about terminology. The problems investigated in this dissertation have a rich history that spans multiple research areas including statistics, operations research, and computer science. With such a storied history spanning decades and departments, it's perhaps no surprise that the terminology used in different domains is not standardized, nor has it been standard across time within each domain. The problem of how to sequentially choose an action from a set of potential actions with unknown rewards where we only observe the outcome for the action we chose is known as the "multi-armed bandit" (MAB) problem. The name is a reference to one-armed bandits, a slang term for slot machines that first appeared in, as far as I am aware, in \cite{bush1953stochastic}. While this is the first paper the authors know of that uses the term, it seems likely that the term was in circulation before this publication because the authors didn't feel the need to define their terminology. Two decades before this problem was named the MAB problem, William R. Thompson proposed a heuristic solution for the two-arm case which adapts the randomization probabilities based on the posterior probability that the arm's mean reward is greatest \cite{thompson1933likelihood}. Related work in this interim period simply referred to the problem under the broad heading of "sequential design" \citep{wald1947sequential, robbins1952some}. When the response is a function of covariates and the action, instead of simply the selected action, the problem is most commonly called the contextual multi-armed bandit (CMAB) problem \citep{woodroofe1979one, langford2007epoch}. 

MAB and CMAB are the standard terminologies in computer science and operations research, but recent statistical work does not have a common term for these problem setups. The methods are often described by the term response-adaptive randomization (RAR) to refer to the case where the treatment assignment only depends on past actions and responses, and covariate-adjusted response-adaptive (CARA) when covariate information is included \citep{hu2006theory}. Depending on the author, response-adaptive randomization may be used as an umbrella term for any randomization method that uses past response information, while others reserve the term for only those methods that use response but not covariate information \citep{hu2006theory}. The even broader term ``adaptive randomization'' is also in use \cite{thall2015statistical}. This is far from an exhaustive list of the terms authors have used to describe these problems, but we hope this historical aside will serve as a useful orientation for those who would like to read more on the subject in different fields.

I will use the terms multi-armed bandit (MAB) and contextual multi-armed bandit (CMAB) when referring to the respective problem formulations because of the long history of the problem under these names. The terms action, arm, intervention, and treatment will be used interchangeably throughout the dissertation to refer to the treatment assignment. Similarly context may be used in place of covariate information, and reward in place of response when a larger response implies a better outcome.

\subsubsection{Landau Notation}

Algorithms and randomization methods may be compared in terms of how quickly the regret grows as a function of the number of arms and study participants. To describe these bounds we'll use Landau notation or ``big o''-notation to characterize the rate that functions grow. For functions $f, g$ that map the natural numbers to the non-negative reals, we say that $f(n)$ is $O\left(g(n)\right)$ if and only if $\limsup_{n \to \infty}\frac{f(n)}{g(n)} < \infty$. While constants may differ and smaller order terms may differ, the leading term in $f(n)$ can't grow at a faster rate than the leading term in $g(n)$. We say that $f(n)$ is $\Omega \left(g(n)\right)$ if and only if $\liminf_{n \to \infty}\frac{f(n)}{g(n)} > 0$.